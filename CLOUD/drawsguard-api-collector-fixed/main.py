#!/usr/bin/env python3
"""
DrawsGuard API Collector - Real-time Telegram Push Version
å®æ—¶Telegramæ¨é€ç‰ˆæœ¬:æ¯æœŸå¼€å¥–å³æ—¶æ¨é€åˆ°Telegram

æ ¸å¿ƒåŠŸèƒ½:
- æ™ºèƒ½è°ƒåº¦:æ ¹æ®award_countdownåŠ¨æ€è°ƒæ•´é‡‡é›†é¢‘ç‡
- å®æ—¶æ¨é€:æ¯æœŸå¼€å¥–ç»“æœç«‹å³æ¨é€åˆ°Telegram(çœŸæ­£çš„å¼‚æ­¥)
- é›¶å»¶è¿Ÿ:BackgroundTaskså¼‚æ­¥æ¨é€,é›¶é˜»å¡ä¸»æµç¨‹
- é«˜å¯é æ€§:å¸¦é‡è¯•æœºåˆ¶å’Œå¤±è´¥é€šçŸ¥

è°ƒåº¦é€»è¾‘:
- countdown > 60ç§’:æ­£å¸¸æ¨¡å¼(ç”±Schedulerè§¦å‘)
- 0 < countdown â‰¤ 60ç§’:å¯†é›†æ¨¡å¼(è‡ªåŠ¨è§¦å‘3æ¬¡é¢å¤–é‡‡é›†)
- countdown â‰¤ -300ç§’:èŠ‚èƒ½æ¨¡å¼(è®°å½•ä½†ä¸é¢å¤–æ“ä½œ)

ä½œè€…: 15å¹´æ•°æ®æ¶æ„ä¸“å®¶
æ—¥æœŸ: 2025-10-04
ç‰ˆæœ¬: 5.2.0 - ç»“æ„ä¿®å¤ç‰ˆ
"""

import uuid
import json
import os
from typing import Dict, Any, Optional
import sys

# å°†å½“å‰åº”ç”¨çš„æ ¹ç›®å½•æ·»åŠ åˆ°Pythonæ¨¡å—æœç´¢è·¯å¾„ä¸­
# è¿™æ˜¯ä¸ºäº†ç¡®ä¿åœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½èƒ½æ­£ç¡®æ‰¾åˆ° aac_common å’Œ collector æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


# å†…éƒ¨æ¨¡å—
from collector.upstream_detector import detect_and_handle_upstream_stale
from common.logging_config import setup_dual_logging
from common.bigquery_client import get_bq_client
from common.utils import (
    async_retry,
    sync_retry,
    get_telegram_config,
    update_telegram_push_stats,
    get_last_issue_from_db,
    PC28Exception,
    get_http_status_code,
)

from google.cloud import bigquery, secretmanager, logging as cloud_logging
import requests
import hashlib
from telegram import Bot
from telegram.constants import ParseMode


# Configure logging at the application's entry point
# This will set up handlers for both console and Google Cloud Logging
setup_dual_logging(service_name="drawsguard-api-collector")

# Get a logger for the current module
logger = logging.getLogger(__name__)

app = FastAPI(
    title="DrawsGuard API Collector v5.2",
    description="äº‘ç«¯æ•°æ®é‡‡é›†æœåŠ¡(çœŸæ­£çš„å¼‚æ­¥Telegramæ¨é€ç‰ˆ, ç»“æ„å·²ä¿®å¤)",
    version="5.2.0"
)

# ================= Pydantic Models for Validation =================

class HealthCheckResponse(BaseModel):
    status: str = "ok"

class BigQueryDrawRow(BaseModel):
    period: str = Field(..., description="æœŸå·")
    timestamp: datetime = Field(..., description="å¼€å¥–æ—¶é—´ (UTC)")
    numbers: List[int] = Field(..., min_items=3, max_items=3, description="å¼€å¥–å·ç ")
    sum_value: int = Field(..., ge=0, le=27, description="å’Œå€¼")
    big_small: str = Field(..., pattern="^(BIG|SMALL)$", description="å¤§å°")
    odd_even: str = Field(..., pattern="^(ODD|EVEN)$", description="å¥‡å¶")
    next_issue: str | None = Field(None, description="ä¸‹æœŸæœŸå·")
    next_time: datetime | None = Field(None, description="ä¸‹æœŸæ—¶é—´ (UTC)")
    award_countdown: int | None = Field(None, description="å¼€å¥–å€’è®¡æ—¶")
    api_server_time: datetime | None = Field(None, description="APIæœåŠ¡å™¨æ—¶é—´ (UTC)")
    clock_drift_ms: int | None = Field(None, description="æ—¶é’Ÿæ¼‚ç§»(æ¯«ç§’)")
    created_at: datetime = Field(..., description="è®°å½•åˆ›å»ºæ—¶é—´ (UTC)")
    updated_at: datetime = Field(..., description="è®°å½•æ›´æ–°æ—¶é—´ (UTC)")


# ================= Global Clients and Config =================

# åˆå§‹åŒ–GCPå®¢æˆ·ç«¯(æ‡’åŠ è½½)
_bq_client = None
_secret_client = None
_logging_client = None
_cloud_logger = None

# æ—¶åŒºé…ç½®
SHANGHAI_TZ = pytz.timezone('Asia/Shanghai')
UTC_TZ = pytz.utc

# æ™ºèƒ½è°ƒåº¦é…ç½® - ä¼˜åŒ–å»¶è¿Ÿ
INTENSIVE_MODE_THRESHOLD = 60  # å¼€å¥–å‰60ç§’è¿›å…¥å¯†é›†æ¨¡å¼
INTENSIVE_INTERVALS = [10, 10, 10]  # å¯†é›†æ¨¡å¼:æ¯10ç§’é‡‡é›†ä¸€æ¬¡(å‡å°‘å»¶è¿Ÿ)
ENERGY_SAVE_THRESHOLD = -300  # å¼€å¥–å5åˆ†é’Ÿè¿›å…¥èŠ‚èƒ½æ¨¡å¼(è®°å½•ä½†ä¸é¢å¤–æ“ä½œ)

# å»¶è¿Ÿä¼˜åŒ–é…ç½®
MAX_ACCEPTABLE_DELAY = 30  # æœ€å¤§å¯æ¥å—å»¶è¿Ÿ30ç§’
DELAY_OPTIMIZATION_THRESHOLD = 45  # è¶…è¿‡45ç§’æ—¶å¯ç”¨é¢å¤–é‡‡é›†

# Telegramæ¨é€ä¼˜åŒ–é…ç½®
TELEGRAM_PUSH_CONFIG = {
    "max_retries": 3,                    # æœ€å¤§é‡è¯•æ¬¡æ•°
    "timeout_seconds": 5,               # è¯·æ±‚è¶…æ—¶æ—¶é—´
    "retry_backoff_base": 2,            # é‡è¯•é€€é¿åŸºæ•°(æŒ‡æ•°é€€é¿)
    "failure_notification_enabled": True, # å¯ç”¨å¤±è´¥é€šçŸ¥
    "async_mode": True,                 # å¼‚æ­¥æ¨¡å¼å¼€å…³
    "performance_monitoring": True      # æ€§èƒ½ç›‘æ§å¼€å…³
}

# å…¨å±€æ€§èƒ½ç»Ÿè®¡
_telegram_push_stats = {
    "total_pushes": 0,
    "successful_pushes": 0,
    "failed_pushes": 0,
    "avg_processing_time": 0.0,
    "max_processing_time": 0.0,
    "min_processing_time": float('inf'),
    "last_push_timestamp": None
}

def get_bq_client():
    """è·å–BigQueryå®¢æˆ·ç«¯(æ‡’åŠ è½½)"""
    global _bq_client
    if _bq_client is None:
        _bq_client = bigquery.Client(
            project='wprojectl',
            location='us-central1'
        )
    return _bq_client

def get_secret_client():
    """è·å–Secret Managerå®¢æˆ·ç«¯(æ‡’åŠ è½½)"""
    global _secret_client
    if _secret_client is None:
        _secret_client = secretmanager.SecretManagerServiceClient()
    return _secret_client

def get_cloud_logger():
    """è·å–Cloud Loggingå®¢æˆ·ç«¯(æ‡’åŠ è½½)"""
    global _logging_client, _cloud_logger
    if _cloud_logger is None:
        _logging_client = cloud_logging.Client(project='wprojectl')
        _cloud_logger = _logging_client.logger('drawsguard-api-collector-smart')
    return _cloud_logger


def update_telegram_push_stats(processing_time: float, success: bool):
    """
    æ›´æ–°Telegramæ¨é€æ€§èƒ½ç»Ÿè®¡

    Args:
        processing_time: å¤„ç†è€—æ—¶(ç§’)
        success: æ˜¯å¦æˆåŠŸ
    """
    global _telegram_push_stats

    if TELEGRAM_PUSH_CONFIG["performance_monitoring"]:
        _telegram_push_stats["total_pushes"] += 1

        if success:
            _telegram_push_stats["successful_pushes"] += 1
        else:
            _telegram_push_stats["failed_pushes"] += 1

        # æ›´æ–°æ—¶é—´ç»Ÿè®¡
        _telegram_push_stats["max_processing_time"] = max(
            _telegram_push_stats["max_processing_time"], processing_time
        )
        _telegram_push_stats["min_processing_time"] = min(
            _telegram_push_stats["min_processing_time"], processing_time
        )

        # è®¡ç®—å¹³å‡å€¼
        total_time = (
            _telegram_push_stats["avg_processing_time"] *
            (_telegram_push_stats["total_pushes"] - 1) +
            processing_time
        )
        _telegram_push_stats["avg_processing_time"] = total_time / _telegram_push_stats["total_pushes"]
        _telegram_push_stats["last_push_timestamp"] = datetime.now(SHANGHAI_TZ).isoformat()


def get_telegram_push_stats() -> Dict[str, Any]:
    """è·å–Telegramæ¨é€æ€§èƒ½ç»Ÿè®¡"""
    return _telegram_push_stats.copy()

@sync_retry()
def get_api_key():
    """ä»Secret Managerè·å–APIå¯†é’¥"""
    try:
        client = get_secret_client()
        name = "projects/wprojectl/secrets/pc28-api-key/versions/latest"
        response = client.access_secret_version(request={"name": name})
        return response.payload.data.decode("UTF-8")
    except Exception as e:
        logger.error(f"è·å–APIå¯†é’¥å¤±è´¥: {e}")
        raise

@sync_retry()
def get_telegram_config():
    """ä»Secret Managerè·å–Telegramé…ç½®"""
    try:
        client = get_secret_client()

        # è·å–bot token
        token_name = "projects/wprojectl/secrets/telegram-bot-token/versions/latest"
        token_response = client.access_secret_version(request={"name": token_name})
        bot_token = token_response.payload.data.decode("UTF-8").strip()  # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œ

        # è·å–chat id
        chat_name = "projects/wprojectl/secrets/telegram-chat-id/versions/latest"
        chat_response = client.access_secret_version(request={"name": chat_name})
        chat_id = chat_response.payload.data.decode("UTF-8").strip()  # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦å’Œæ¢è¡Œ

        return bot_token, chat_id
    except Exception as e:
        logger.warning(f"âš ï¸ è·å–Telegramé…ç½®å¤±è´¥: {e}")
        return None, None

async def send_telegram_lottery_result_async(period: str, numbers: list, sum_value: int, big_small: str, odd_even: str):
    """
    å¼‚æ­¥å‘é€å¼€å¥–ç»“æœåˆ°Telegram(ä½¿ç”¨BackgroundTasks)

    Args:
        period: æœŸå·
        numbers: å¼€å¥–å·ç (3ä¸ªæ•°å­—)
        sum_value: å’Œå€¼
        big_small: å¤§å°
        odd_even: å¥‡å¶
    """
    start_time = time.time()
    trace_id = f"telegram_{period}_{int(start_time)}"

    try:
        logger.info(f"ğŸ”„ å¼€å§‹å¼‚æ­¥Telegramæ¨é€: æœŸå·{period} (trace_id: {trace_id})")

        # è·å–Telegramé…ç½®
        bot_token, chat_id = get_telegram_config()
        if not bot_token or not chat_id:
            logger.info(f"â„¹ï¸ Telegramæœªé…ç½®,è·³è¿‡æ¨é€ (trace_id: {trace_id})")
            return

        # æ„é€ ç²¾ç¾çš„å¼€å¥–æ¶ˆæ¯
        message = (
            f"ğŸ° **PC28å¼€å¥–æ’­æŠ¥**\n\n"
            f"ğŸ“… æœŸå·: `{period}`\n"
            f"ğŸ² å·ç : `{numbers[0]} + {numbers[1]} + {numbers[2]}`\n"
            f"â• å’Œå€¼: `{sum_value}`\n"
            f"ğŸ“Š å¤§å°: `{big_small}`\n"
            f"ğŸ“ˆ å¥‡å¶: `{odd_even}`\n\n"
            f"â° æ—¶é—´: {datetime.now(SHANGHAI_TZ).strftime('%Y-%m-%d %H:%M:%S')} (åŒ—äº¬æ—¶é—´)\n"
            f"ğŸ”— è¿½è¸ªID: `{trace_id}`"
        )

        # å¼‚æ­¥å‘é€åˆ°Telegram(å¸¦é‡è¯•æœºåˆ¶)
        await _send_telegram_with_retry(bot_token, chat_id, message, period, trace_id)

        processing_time = time.time() - start_time
        logger.info(f"âœ… Telegramå¼€å¥–æ¨é€æˆåŠŸ: æœŸå·{period} (è€—æ—¶: {processing_time:.2f}ç§’, trace_id: {trace_id})")

        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        update_telegram_push_stats(processing_time, success=True)

    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(
            f"âŒ Telegramæ¨é€å¤±è´¥: æœŸå·{period} (è€—æ—¶: {processing_time:.2f}ç§’, trace_id: {trace_id}, é”™è¯¯: {e})"
        )

        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡(å¤±è´¥)
        update_telegram_push_stats(processing_time, success=False)

        # å‘é€å¤±è´¥é€šçŸ¥åˆ°å¤‡ç”¨æ¸ é“(å¯é€‰)
        await _send_failure_notification(period, str(e), trace_id)


async def send_stale_upstream_alert_async(alert: dict):
    """
    å¼‚æ­¥å‘é€ä¸Šæ¸¸åœæ›´è­¦å‘Šåˆ°Telegram
    """
    start_time = time.time()
    period = alert.get("stale_period")
    trace_id = f"stale_alert_{period}_{int(start_time)}"

    try:
        # 1. æ ¼å¼åŒ–å‘Šè­¦æ¶ˆæ¯ (å®‰å…¨æ‹¼æ¥æ–¹å¼)
        msg_title = "[WARNING] Upstream STALE - rijb.api.storeapi.net"
        msg_time = f"Time: {alert.get('detected_at')}"
        msg_collector = f"Collector: {alert.get('collector')}"
        msg_observed = f"Observed: è¿ç»­ {alert.get('consecutive_count')} æ¬¡è¿”å›æœŸå· {period}ï¼ˆå·²å­˜åœ¨ DBï¼ŒMERGE skipï¼‰"
        msg_action = "Action: è¯·è”ç³»ä¸Šæ¸¸ç¡®è®¤æ˜¯å¦åœæ­¢æ›´æ–°ï¼›è‹¥ 4 å°æ—¶å†…ä»æœªæ¢å¤ï¼Œå°† escalate ä¸º P0ã€‚"
        message = f"{msg_title}\n\n{msg_time}\n{msg_collector}\n{msg_observed}\n\n{msg_action}"

        logger.info(f"å‡†å¤‡å‘é€ä¸Šæ¸¸åœæ›´è­¦å‘Š: æœŸå·={period} (trace_id: {trace_id})")

        # 2. è·å–é…ç½®å¹¶å‘é€
        bot_token, chat_id = get_telegram_config()
        if not bot_token or not chat_id:
            logger.info(f"â„¹ï¸ Telegramæœªé…ç½®,è·³è¿‡ä¸Šæ¸¸åœæ›´è­¦å‘Šæ¨é€ (trace_id: {trace_id})")
            return

        await _send_telegram_with_retry(bot_token, chat_id, message, str(period), trace_id)

        processing_time = time.time() - start_time
        logger.info(f"âœ… ä¸Šæ¸¸åœæ›´è­¦å‘Šæ¨é€æˆåŠŸ: æœŸå·={period} (è€—æ—¶: {processing_time:.2f}ç§’, trace_id: {trace_id})")

    except Exception as e:
        logger.error(f"âŒ ä¸Šæ¸¸åœæ›´è­¦å‘Šæ¨é€å¤±è´¥: æœŸå·={period} (trace_id: {trace_id}, é”™è¯¯: {e})")


@async_retry(max_retries=3)
async def _send_telegram_with_retry(bot_token: str, chat_id: str, message: str, period: str, trace_id: str):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„Telegramå‘é€(å¼‚æ­¥)

    Args:
        bot_token: Telegramæœºå™¨äººä»¤ç‰Œ
        chat_id: èŠå¤©ID
        message: æ¶ˆæ¯å†…å®¹
        period: æœŸå·(ç”¨äºæ—¥å¿—)
        trace_id: è¿½è¸ªID
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    """
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    payload = {
        'chat_id': chat_id,
        'text': message,
        'parse_mode': 'Markdown',
        'disable_notification': False
    }

    try:
        # ä½¿ç”¨aiohttpè¿›è¡Œå¼‚æ­¥HTTPè¯·æ±‚(å¦‚æœå¯ç”¨)
        import aiohttp

        timeout = aiohttp.ClientTimeout(total=5)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.post(url, json=payload) as response:
                response.raise_for_status()
                result = await response.json()
                if result.get('ok'):
                    logger.info(f"ğŸ“± Telegramæ¨é€æˆåŠŸ: æœŸå·{period} (trace_id: {trace_id})")
                    return True
                else:
                    logger.warning(f"âš ï¸ Telegram APIè¿”å›éæˆåŠŸ: {result} (trace_id: {trace_id})")
                    raise Exception(f"Telegram API Error: {result}") from None

    except ImportError:
        # é™çº§ä½¿ç”¨requests(åŒæ­¥,ä½†ä»ç„¶æ˜¯éé˜»å¡çš„)
        logger.info(f"ğŸ”„ ä½¿ç”¨åŒæ­¥requestså‘é€Telegram (trace_id: {trace_id})")
        response = await asyncio.to_thread(requests.post, url, json=payload, timeout=5)
        response.raise_for_status()

        result = response.json()
        if result.get('ok'):
            logger.info(f"ğŸ“± Telegramæ¨é€æˆåŠŸ: æœŸå·{period}, (trace_id: {trace_id})")
            return True
        else:
            logger.warning(f"âš ï¸ Telegram APIè¿”å›éæˆåŠŸ: {result} (trace_id: {trace_id})")
            raise Exception(f"Telegram API Error: {result}") from None


async def _send_failure_notification(period: str, error_msg: str, trace_id: str):
    """
    å‘é€æ¨é€å¤±è´¥é€šçŸ¥åˆ°å¤‡ç”¨æ¸ é“(å¼‚æ­¥)

    Args:
        period: æœŸå·
        error_msg: é”™è¯¯ä¿¡æ¯
        trace_id: è¿½è¸ªID
    """
    try:
        # è¿™é‡Œå¯ä»¥é›†æˆå…¶ä»–é€šçŸ¥æ–¹å¼,å¦‚é‚®ä»¶ã€Slackç­‰
        logger.warning(f"ğŸš¨ Telegramæ¨é€å¤±è´¥é€šçŸ¥: æœŸå·{period}, é”™è¯¯: {error_msg} (trace_id: {trace_id})")

        # ç¤ºä¾‹:å‘é€åˆ°Cloud Logging(ç»“æ„åŒ–æ—¥å¿—)
        cloud_logger = get_cloud_logger()
        cloud_logger.log_struct({
            "severity": "WARNING",
            "message": "Telegramæ¨é€å¤±è´¥",
            "period": period,
            "error": error_msg,
            "trace_id": trace_id,
            "notification_type": "telegram_failure"
        })

    except Exception as e:
        logger.error(f"âŒ å‘é€å¤±è´¥é€šçŸ¥ä¹Ÿå¤±è´¥äº†: {e} (trace_id: {trace_id})")


async def send_prediction_result_async(orders: List[Dict[str, Any]], period: str = None):
    """
    å¼‚æ­¥å‘é€é¢„æµ‹ç»“æœåˆ°Telegram(ä½¿ç”¨BackgroundTasks)

    Args:
        orders: é¢„æµ‹è®¢å•åˆ—è¡¨
        period: æœŸå·(å¯é€‰,ç”¨äºæ—¥å¿—)
    """
    start_time = time.time()
    trace_id = f"prediction_{period or 'batch'}_{int(start_time)}"

    try:
        logger.info(f"ğŸ”® å¼€å§‹å¼‚æ­¥é¢„æµ‹ç»“æœæ¨é€: è®¢å•æ•°{len(orders)} (trace_id: {trace_id})")

        # è·å–Telegramé…ç½®
        bot_token, chat_id = get_telegram_config()
        if not bot_token or not chat_id:
            logger.info(f"â„¹ï¸ Telegramæœªé…ç½®,è·³è¿‡é¢„æµ‹ç»“æœæ¨é€ (trace_id: {trace_id})")
            return

        # æ„é€ é¢„æµ‹ç»“æœæ¶ˆæ¯
        message = build_prediction_message(orders, period, trace_id)

        # å¼‚æ­¥å‘é€é¢„æµ‹ç»“æœ
        await _send_telegram_with_retry(bot_token, chat_id, message, period or "batch", trace_id)

        processing_time = time.time() - start_time
        logger.info(f"âœ… é¢„æµ‹ç»“æœæ¨é€æˆåŠŸ: {len(orders)}ä¸ªè®¢å• (è€—æ—¶: {processing_time:.2f}ç§’, trace_id: {trace_id})")

        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        update_telegram_push_stats(processing_time, success=True)

    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"âŒ é¢„æµ‹ç»“æœæ¨é€å¤±è´¥: {len(orders)}ä¸ªè®¢å• (è€—æ—¶: {processing_time:.2f}ç§’, trace_id: {trace_id}, é”™è¯¯: {e})")

        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡(å¤±è´¥)
        update_telegram_push_stats(processing_time, success=False)

        # å‘é€å¤±è´¥é€šçŸ¥
        await _send_failure_notification(period or "batch", str(e), trace_id)


def build_prediction_message(orders: List[Dict[str, Any]], period: str = None, trace_id: str = None) -> str:
    """
    æ„å»ºé¢„æµ‹ç»“æœæ¶ˆæ¯

    Args:
        orders: é¢„æµ‹è®¢å•åˆ—è¡¨
        period: æœŸå·
        trace_id: è¿½è¸ªID

    Returns:
        æ ¼å¼åŒ–çš„æ¶ˆæ¯æ–‡æœ¬
    """
    if not orders:
        return "ğŸ¤– **PC28é¢„æµ‹ç»“æœ**\n\nğŸ“­ æ— é¢„æµ‹è®¢å•ç”Ÿæˆ\n\nâ° è¯·ç­‰å¾…ä¸‹ä¸€æœŸé¢„æµ‹"

    # æŒ‰å¸‚åœºåˆ†ç»„ç»Ÿè®¡
    market_stats = {}
    for order in orders:
        market = order.get('market', 'unknown')
        if market not in market_stats:
            market_stats[market] = []
        market_stats[market].append(order)

    # æ„å»ºæ¶ˆæ¯
    lines = ["ğŸ¤– **PC28é¢„æµ‹ç»“æœ**"]

    if period:
        lines.append(f"\nğŸ“… é¢„æµ‹æœŸå·: `{period}`")

    lines.append("\nğŸ“Š é¢„æµ‹è®¢å•è¯¦æƒ…:")
    lines.append(f"ğŸ“ˆ æ€»è®¢å•æ•°: `{len(orders)}`")

    for market, market_orders in market_stats.items():
        lines.append(f"\nğŸª å¸‚åœº: `{market.upper()}`")
        lines.append(f"ğŸ“‹ è®¢å•æ•°é‡: `{len(market_orders)}`")

        # æ˜¾ç¤ºå‰3ä¸ªè®¢å•çš„è¯¦ç»†ä¿¡æ¯
        for i, order in enumerate(market_orders[:3]):
            stake = order.get('stake_u', 0)
            prob = order.get('p_win', 0)
            ev = order.get('ev', 0)
            lines.append(f"  {i+1}. æ¦‚ç‡: `{prob:.2%}`, EV: `{ev:.2%}`, é‡‘é¢: `{stake}`U")

        if len(market_orders) > 3:
            lines.append(f"  ... è¿˜æœ‰ {len(market_orders) - 3} ä¸ªè®¢å•")

    lines.append(f"\nâ° æ—¶é—´: {datetime.now(SHANGHAI_TZ).strftime('%Y-%m-%d %H:%M:%S')} (åŒ—äº¬æ—¶é—´)")

    if trace_id:
        lines.append(f"ğŸ”— è¿½è¸ªID: `{trace_id}`")

    return "\n".join(lines)


# åŒæ­¥ç‰ˆæœ¬(ç”¨äºå…¼å®¹æ€§)
def send_prediction_result(orders: List[Dict[str, Any]], period: str = None):
    """
    åŒæ­¥å‘é€é¢„æµ‹ç»“æœæ¶ˆæ¯(ç”¨äºéFastAPIç¯å¢ƒ)

    æ³¨æ„: æ­¤å‡½æ•°æ˜¯é˜»å¡çš„,ä»…ç”¨äºå…¼å®¹æ€§
    """
    try:
        bot_token, chat_id = get_telegram_config()
        if not bot_token or not chat_id:
            logger.info("â„¹ï¸ Telegramæœªé…ç½®,è·³è¿‡é¢„æµ‹ç»“æœæ¨é€")
            return False

        # æ„é€ é¢„æµ‹ç»“æœæ¶ˆæ¯
        message = build_prediction_message(orders, period)

        # å‘é€åˆ°Telegram
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            'chat_id': chat_id,
            'text': message,
            'parse_mode': 'Markdown',
            'disable_notification': False
        }

        response = requests.post(url, json=payload, timeout=5)
        response.raise_for_status()

        logger.info(f"âœ… é¢„æµ‹ç»“æœæ¨é€æˆåŠŸ: {len(orders)}ä¸ªè®¢å•")
        return True

    except Exception as e:
        logger.error(f"âŒ é¢„æµ‹ç»“æœæ¨é€å¤±è´¥: {e}")
        return False


# åŒæ­¥ç‰ˆæœ¬(ç”¨äºå…¼å®¹æ€§)
def send_telegram_lottery_result(period: str, numbers: list, sum_value: int, big_small: str, odd_even: str):
    """
    åŒæ­¥å‘é€Telegramæ¶ˆæ¯(ç”¨äºéFastAPIç¯å¢ƒ)

    æ³¨æ„: æ­¤å‡½æ•°æ˜¯é˜»å¡çš„,ä»…ç”¨äºå…¼å®¹æ€§
    """
    try:
        bot_token, chat_id = get_telegram_config()
        if not bot_token or not chat_id:
            logger.info("â„¹ï¸ Telegramæœªé…ç½®,è·³è¿‡æ¨é€")
            return False

        # æ„é€ ç²¾ç¾çš„å¼€å¥–æ¶ˆæ¯
        message = (
            f"ğŸ° **PC28å¼€å¥–æ’­æŠ¥**\n\n"
            f"ğŸ“… æœŸå·: `{period}`\n"
            f"ğŸ² å·ç : `{numbers[0]} + {numbers[1]} + {numbers[2]}`\n"
            f"â• å’Œå€¼: `{sum_value}`\n"
            f"ğŸ“Š å¤§å°: `{big_small}`\n"
            f"ğŸ“ˆ å¥‡å¶: `{odd_even}`\n\n"
            f"â° æ—¶é—´: {datetime.now(SHANGHAI_TZ).strftime('%Y-%m-%d %H:%M:%S')} (åŒ—äº¬æ—¶é—´)"
        )

        # å‘é€åˆ°Telegram
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            'chat_id': chat_id,
            'text': message,
            'parse_mode': 'Markdown',
            'disable_notification': False
        }

        response = requests.post(url, json=payload, timeout=5)
        response.raise_for_status()

        logger.info(f"âœ… Telegramå¼€å¥–æ¨é€æˆåŠŸ: æœŸå·{period}")
        return True

    except Exception as e:
        logger.error(f"âŒ Telegramæ¨é€å¤±è´¥: {e}")
        return False

def generate_sign(params: dict, api_key: str) -> str:
    """ç”ŸæˆAPIç­¾å"""
    filtered_params = {k: v for k, v in params.items() if v is not None and v != ''}
    sorted_keys = sorted(filtered_params.keys())
    sign_string = ''.join([f"{k}{filtered_params[k]}" for k in sorted_keys])
    sign_string += api_key
    return hashlib.md5(sign_string.encode('utf-8')).hexdigest()


@sync_retry(max_retries=3)
def call_api_with_retry(api_url: str, params: dict, api_key: str) -> dict:
    """è°ƒç”¨API(å¸¦é‡è¯•æœºåˆ¶å’Œç­¾å)"""
    try:
        # 1. ä¸ºæœ¬æ¬¡è¯·æ±‚åŠ å…¥æ—¶é—´æˆ³ï¼Œè¿™æ˜¯ç­¾åçš„ä¸€éƒ¨åˆ†
        params['time'] = int(time.time())

        # 2. ç”Ÿæˆç­¾å
        sign = generate_sign(params, api_key)

        # 3. å°†ç­¾ååŠ å…¥åˆ°æœ€ç»ˆçš„è¯·æ±‚å‚æ•°ä¸­
        params['sign'] = sign

        logger.info(f"APIè°ƒç”¨: params={params}")

        response = requests.get(
            api_url,
            params=params,
            timeout=30,
            headers={'User-Agent': 'DrawsGuard/4.0-Smart'}
        )
        response.raise_for_status()
        data = response.json()
        logger.info("âœ… APIè°ƒç”¨æˆåŠŸ")
        return data

    except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout) as e:
        logger.warning(f"âš ï¸ APIè°ƒç”¨è¶…æ—¶: {str(e)}")
        raise  # Re-raise to be caught by the retry decorator

    except Exception as e:
        logger.error(f"APIè°ƒç”¨å‡ºé”™: {str(e)}")
        raise  # Re-raise to be caught by the retry decorator


def _insert_draw_with_merge(bq_client: bigquery.Client, row: Dict[str, Any]) -> str:
    """
    Inserts a single draw record into BigQuery using a MERGE statement for atomic upsert.

    Args:
        bq_client: The BigQuery client.
        row: A dictionary representing the row to be inserted.

    Returns:
        A string indicating the result: 'inserted' or 'skipped'.
    """
    table_id = "wprojectl.drawsguard.draws"

    # Using MERGE for atomic "insert if not exists"
    merge_sql = f"""
    MERGE `{table_id}` T
    USING (
        SELECT
            @period AS period,
            @timestamp AS timestamp,
            @numbers AS numbers,
            @sum_value AS sum_value,
            @big_small AS big_small,
            @odd_even AS odd_even,
            @next_issue AS next_issue,
            @next_time AS next_time,
            @award_countdown AS award_countdown,
            @api_server_time AS api_server_time,
            @clock_drift_ms AS clock_drift_ms,
            @created_at AS created_at,
            @updated_at AS updated_at
    ) S
    ON T.period = S.period
    WHEN NOT MATCHED THEN
      INSERT (
          period, timestamp, numbers, sum_value, big_small, odd_even,
          next_issue, next_time, award_countdown, api_server_time,
          clock_drift_ms, created_at, updated_at
      )
      VALUES (
          S.period, S.timestamp, S.numbers, S.sum_value, S.big_small, S.odd_even,
          S.next_issue, S.next_time, S.award_countdown, S.api_server_time,
          S.clock_drift_ms, S.created_at, S.updated_at
      )
    """

    # Pydantic model guarantees correct types, but BQ API needs specific formats
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("period", "STRING", row["period"]),
            bigquery.ScalarQueryParameter("timestamp", "TIMESTAMP", row["timestamp"]),
            bigquery.ArrayQueryParameter("numbers", "INT64", row["numbers"]),
            bigquery.ScalarQueryParameter("sum_value", "INT64", row["sum_value"]),
            bigquery.ScalarQueryParameter("big_small", "STRING", row["big_small"]),
            bigquery.ScalarQueryParameter("odd_even", "STRING", row["odd_even"]),
            bigquery.ScalarQueryParameter("next_issue", "STRING", row["next_issue"]),
            bigquery.ScalarQueryParameter("next_time", "TIMESTAMP", row["next_time"]),
            bigquery.ScalarQueryParameter("award_countdown", "INT64", row["award_countdown"]),
            bigquery.ScalarQueryParameter("api_server_time", "TIMESTAMP", row["api_server_time"]),
            bigquery.ScalarQueryParameter("clock_drift_ms", "INT64", row["clock_drift_ms"]),
            bigquery.ScalarQueryParameter("created_at", "TIMESTAMP", row["created_at"]),
            bigquery.ScalarQueryParameter("updated_at", "TIMESTAMP", row["updated_at"]),
        ]
    )

    query_job = bq_client.query(merge_sql, job_config=job_config)
    query_job.result()  # Wait for the job to complete

    if query_job.num_dml_affected_rows > 0:
        return "inserted"
    else:
        return "skipped"

def parse_and_insert_data(api_data: dict, bq_client: bigquery.Client, cloud_logger, background_tasks: BackgroundTasks, collection_mode: str = "normal") -> dict:
    """
    è§£æAPIæ•°æ®å¹¶æ’å…¥BigQuery
    
    Args:
        api_data: APIè¿”å›æ•°æ®
        bq_client: BigQueryå®¢æˆ·ç«¯
        cloud_logger: Cloud Loggingå®¢æˆ·ç«¯
        collection_mode: é‡‡é›†æ¨¡å¼(normal/intensive/energy_save)
        
    Returns:
        æ‰§è¡Œç»“æœç»Ÿè®¡
    """
    try:
        # æå–æ•°æ®
        curent = api_data.get('retdata', {}).get('curent', {})
        next_data = api_data.get('retdata', {}).get('next', {})

        # åŸºç¡€å­—æ®µ
        period = str(curent.get('long_issue', ''))
        kjtime_str = curent.get('kjtime', '')
        numbers = curent.get('number', [])

        # ğŸ›¡ï¸ ä¸Šæ¸¸åœæ›´æ£€æµ‹å™¨ (Upstream Stale Detector)
        # åœ¨è§£ææ ¸å¿ƒæ•°æ®åç«‹åˆ»æ‰§è¡Œ,ä½†ä¸é˜»å¡ä¸»æµç¨‹
        try:
            # ä»…å½“æœŸå·æœ‰æ•ˆæ—¶æ‰è¿è¡Œæ£€æµ‹å™¨
            if period and period.isdigit():
                returned_period_int = int(period)
                response_json_str = json.dumps(api_data)

                # åŒæ­¥æ‰§è¡Œæ£€æµ‹å’ŒBQæ—¥å¿—è®°å½•
                alert = detect_and_handle_upstream_stale(
                    collector_name="drawsguard-api-collector",
                    returned_period=returned_period_int,
                    response_json=response_json_str
                )

                # å¦‚æœæ£€æµ‹åˆ°åœæ›´,åˆ™å¼‚æ­¥å‘é€è­¦å‘Š
                if alert:
                    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°ä¸Šæ¸¸åœæ›´,å·²ç”Ÿæˆè­¦å‘Š: {alert}")
                    background_tasks.add_task(send_stale_upstream_alert_async, alert)
            else:
                logger.debug(f"è·³è¿‡ä¸Šæ¸¸åœæ›´æ£€æµ‹ï¼Œæ— æ•ˆçš„æœŸå·: '{period}'")

        except (ValueError, TypeError) as e:
            logger.warning(f"æ— æ³•ä¸ºä¸Šæ¸¸åœæ›´æ£€æµ‹å™¨å‡†å¤‡æ•°æ®: period='{period}', error={e}")
        except Exception as e:
            logger.error(f"ä¸Šæ¸¸åœæ›´æ£€æµ‹å™¨ä»»åŠ¡è°ƒåº¦å¤±è´¥: {e}", exc_info=True)

        # æ–°å¢å­—æ®µ
        next_issue = str(next_data.get('next_issue', ''))
        next_time_str = next_data.get('next_time', '')
        award_countdown = int(next_data.get('award_time', 0))

        # curtimeå­—æ®µå¤„ç†(100%å­—æ®µåˆ©ç”¨ç‡)- ä¼˜åŒ–æ—¶é’Ÿæ¼‚ç§»æ£€æµ‹
        api_curtime = api_data.get('curtime', 0)
        local_timestamp = datetime.now(UTC_TZ)

        if api_curtime:
            api_server_time = datetime.fromtimestamp(int(api_curtime), tz=UTC_TZ)
            raw_drift_ms = int((local_timestamp.timestamp() - api_server_time.timestamp()) * 1000)

            # ä¼˜åŒ–æ—¶é’Ÿæ¼‚ç§»è®¡ç®—:è€ƒè™‘ç½‘ç»œå»¶è¿Ÿ,å‡å°‘è¯¯æŠ¥
            # å‡è®¾ç½‘ç»œå»¶è¿Ÿåœ¨0-1000msä¹‹é—´,æ—¶é’Ÿæ¼‚ç§»ä¸åº”è¶…è¿‡Â±2000ms
            if abs(raw_drift_ms) <= 2000:
                clock_drift_ms = raw_drift_ms
            else:
                # è¶…è¿‡é˜ˆå€¼æ—¶,å–ç½‘ç»œå»¶è¿Ÿçš„ä¸­ä½æ•°ä½œä¸ºæ¼‚ç§»ä¼°è®¡
                clock_drift_ms = 500 if raw_drift_ms > 0 else -500  # ä¿å®ˆä¼°è®¡
                drift_warning = f"âš ï¸ æ—¶é’Ÿæ¼‚ç§»å¼‚å¸¸: {raw_drift_ms}ms â†’ æ ¡æ­£ä¸º{clock_drift_ms}ms (æœ¬åœ°={local_timestamp.isoformat()}, API={api_server_time.isoformat()})"
                logger.warning(drift_warning)
                cloud_logger.log_text(drift_warning, severity='WARNING')

            # æ—¶é’Ÿæ¼‚ç§»å‘Šè­¦(ä»…å¯¹æ˜¾è‘—æ¼‚ç§»å‘Šè­¦)
            if abs(clock_drift_ms) > 1000:
                drift_info = f"æ—¶é’Ÿæ¼‚ç§»: {clock_drift_ms}ms"
                logger.info(f"â° {drift_info}")
        else:
            api_server_time = None
            clock_drift_ms = None

        if not period or not kjtime_str or not numbers:
            raise ValueError("å…³é”®å­—æ®µç¼ºå¤±")

        # æ—¶é—´è½¬æ¢(ä¸Šæµ·æ—¶åŒº â†’ UTC)
        naive_dt = datetime.strptime(kjtime_str, "%Y-%m-%d %H:%M:%S")
        aware_dt = SHANGHAI_TZ.localize(naive_dt)
        timestamp_utc = aware_dt.astimezone(UTC_TZ)

        # next_timeè½¬æ¢
        if next_time_str:
            next_naive = datetime.strptime(next_time_str, "%Y-%m-%d %H:%M:%S")
            next_aware = SHANGHAI_TZ.localize(next_naive)
            next_time_utc = next_aware.astimezone(UTC_TZ)
        else:
            next_time_utc = None

        # å·ç å¤„ç†
        numbers_int = [int(n) for n in numbers]
        sum_value = sum(numbers_int)
        big_small = "BIG" if sum_value >= 14 else "SMALL"  # ä¼˜åŒ–:ä½¿ç”¨å¤§å†™(ç¬¦åˆTECHNICAL_SPECSè§„èŒƒ)
        odd_even = "EVEN" if sum_value % 2 == 0 else "ODD"  # ä¼˜åŒ–:ä½¿ç”¨å¤§å†™

        # æ„é€ è¡Œæ•°æ®(100%å­—æ®µåˆ©ç”¨ç‡)
        raw_row = {
            "period": period,
            "timestamp": timestamp_utc,
            "numbers": numbers_int,
            "sum_value": sum_value,
            "big_small": big_small,
            "odd_even": odd_even,
            "next_issue": next_issue,
            "next_time": next_time_utc,
            "award_countdown": award_countdown,
            "api_server_time": api_server_time,
            "clock_drift_ms": clock_drift_ms,
            "created_at": local_timestamp,
            "updated_at": local_timestamp,
        }

        # ğŸ›¡ï¸ åœ¨æ’å…¥å‰è¿›è¡Œä¸¥æ ¼çš„æ•°æ®éªŒè¯
        try:
            validated_row = BigQueryDrawRow.model_validate(raw_row)
            # ä½¿ç”¨ç»è¿‡éªŒè¯å’Œåºåˆ—åŒ–çš„æ•°æ®è¿›è¡Œæ’å…¥ï¼Œç¡®ä¿ç±»å‹å®‰å…¨
            row_to_insert = validated_row.model_dump(mode='json')
        except ValidationError as e:
            error_msg = f"æ•°æ®éªŒè¯å¤±è´¥: period={period}, errors={e}"
            logger.error(error_msg)
            cloud_logger.log_text(error_msg, severity='CRITICAL') # æå‡å‘Šè­¦ç­‰çº§
            return {"success": False, "error": "Data validation failed", "details": str(e)}


        # Atomically insert the row using a MERGE statement
        insert_status = _insert_draw_with_merge(bq_client, row_to_insert)

        if insert_status == "skipped":
            duplicate_warning = f"âš ï¸ æœŸå·é‡å¤(å·²é€šè¿‡MERGEè·³è¿‡): period={period}"
            logger.warning(duplicate_warning)
            cloud_logger.log_text(duplicate_warning, severity='WARNING')

            return {
                "success": True,
                "period": period,
                "status": "duplicate_skipped",
                "next_issue": next_issue,
                "award_countdown": award_countdown,
                "collection_mode": collection_mode
            }

        # æ ¹æ®æ¨¡å¼è®°å½•æ—¥å¿—(åŒ…å«æ—¶é’Ÿæ¼‚ç§»ä¿¡æ¯)
        mode_emoji = {"normal": "ğŸŸ¢", "intensive": "ğŸ”´", "energy_save": "ğŸ”µ"}.get(collection_mode, "âšª")
        drift_info = f", drift={clock_drift_ms}ms" if clock_drift_ms is not None else ""
        logger.info(
            f"{mode_emoji} æ•°æ®æ’å…¥æˆåŠŸ [{collection_mode.upper()}]: "
            f"æœŸå·={period}, next={next_issue}, countdown={award_countdown}s{drift_info}"
        )
        cloud_logger.log_text(
            f"[{collection_mode}] period={period}, next={next_issue}, countdown={award_countdown}, clock_drift={clock_drift_ms}ms",
            severity='INFO'
        )

        # ğŸ° å®æ—¶æ¨é€å¼€å¥–ç»“æœåˆ°Telegram(çœŸæ­£çš„å¼‚æ­¥,ä¸é˜»å¡ä¸»æµç¨‹)
        # ä½¿ç”¨BackgroundTaskså®ç°çœŸæ­£çš„éé˜»å¡æ¨é€
        background_tasks.add_task(
            send_telegram_lottery_result_async,
            period,
            numbers_int,
            sum_value,
            big_small,
            odd_even
        )
        logger.info(f"ğŸ“± Telegramæ¨é€ä»»åŠ¡å·²åŠ å…¥åå°é˜Ÿåˆ—: æœŸå·{period} (ä¸»æµç¨‹ç»§ç»­æ‰§è¡Œ)")

        # è¿ç»­æ€§æ£€æŸ¥
        continuity_status = "pass"
        if next_issue:
            expected_next = str(int(period) + 1)
            if next_issue != expected_next:
                warning_msg = f"âš ï¸ æœŸå·ä¸è¿ç»­ï¼å½“å‰={period}, é¢„æœŸ={expected_next}, å®é™…={next_issue}"
                logger.warning(warning_msg)
                cloud_logger.log_text(warning_msg, severity='WARNING')
                continuity_status = "warning"

        return {
            "success": True,
            "period": period,
            "next_issue": next_issue,
            "award_countdown": award_countdown,
            "continuity_check": continuity_status,
            "collection_mode": collection_mode,
            "clock_drift_ms": clock_drift_ms
        }

    except Exception as e:
        error_msg = f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        cloud_logger.log_text(error_msg, severity='ERROR')
        return {"success": False, "error": str(e)}

def schedule_intensive_collection(countdown: int, api_key: str, bq_client, cloud_logger, background_tasks: BackgroundTasks):
    """
    å¯†é›†é‡‡é›†æ¨¡å¼:åœ¨å¼€å¥–å‰60ç§’è§¦å‘é¢å¤–é‡‡é›†(ä¼˜åŒ–å»¶è¿Ÿ)

    Args:
        countdown: å½“å‰è·ç¦»å¼€å¥–çš„ç§’æ•°
        api_key: APIå¯†é’¥
        bq_client: BigQueryå®¢æˆ·ç«¯
        cloud_logger: Cloud Loggingå®¢æˆ·ç«¯
        background_tasks: FastAPIåå°ä»»åŠ¡
    """
    logger.info(f"ğŸ”´ è¿›å…¥å¯†é›†é‡‡é›†æ¨¡å¼(countdown={countdown}ç§’)")
    cloud_logger.log_text(f"è¿›å…¥å¯†é›†é‡‡é›†æ¨¡å¼: countdown={countdown}", severity='INFO')

    # ä¼˜åŒ–é‡‡é›†ç­–ç•¥:åŸºäºå€’è®¡æ—¶åŠ¨æ€è°ƒæ•´é—´éš”
    intervals = []
    if countdown > 50:
        intervals = [10, 10, 10]  # åœ¨50ç§’ã€40ç§’ã€30ç§’æ—¶é‡‡é›†
    elif countdown > 30:
        intervals = [countdown - 30, 10, 10]  # åœ¨30ç§’ã€20ç§’ã€10ç§’æ—¶é‡‡é›†
    elif countdown > 15:
        intervals = [countdown - 15, 10]  # åœ¨15ç§’ã€5ç§’æ—¶é‡‡é›†
    else:
        intervals = [max(countdown - 3, 1)]  # ç«‹å³é‡‡é›†

    logger.info(f"ğŸ“… å¯†é›†é‡‡é›†è®¡åˆ’: {len(intervals)}æ¬¡, é—´éš”={intervals}")

    # æ‰§è¡Œå¯†é›†é‡‡é›†(å¸¦å»¶è¿Ÿç›‘æ§)
    api_url = "https://rijb.api.storeapi.net/api/119/259"
    for i, wait_time in enumerate(intervals):
        time.sleep(wait_time)

        # é‡‡é›†å‰è®°å½•æ—¶é—´
        collect_start = time.time()

        try:
            current_time = str(int(time.time()))
            params = {
                'appid': '45928',
                'format': 'json',
                'time': current_time
            }
            params['sign'] = generate_sign(params, api_key)

            data = call_api_with_retry(api_url, params, api_key)

            if data.get('codeid') == 10000:
                result = parse_and_insert_data(data, bq_client, cloud_logger, background_tasks, collection_mode="intensive")

                # è®¡ç®—é‡‡é›†å»¶è¿Ÿ
                collect_delay = time.time() - collect_start
                logger.info(f"âœ… å¯†é›†é‡‡é›† {i+1}/{len(intervals)} å®Œæˆ (å»¶è¿Ÿ: {collect_delay:.1f}ç§’)")

                # å¦‚æœå»¶è¿Ÿè¿‡å¤§,è®°å½•è­¦å‘Š
                if collect_delay > MAX_ACCEPTABLE_DELAY:
                    delay_warning = f"âš ï¸ å¯†é›†é‡‡é›†å»¶è¿Ÿè¾ƒå¤§: {collect_delay:.1f}ç§’ > {MAX_ACCEPTABLE_DELAY}ç§’"
                    logger.warning(delay_warning)
                    cloud_logger.log_text(delay_warning, severity='WARNING')
            else:
                logger.warning(f"âš ï¸ å¯†é›†é‡‡é›† {i+1}/{len(intervals)} APIè¿”å›é”™è¯¯: {data.get('message')}")

        except Exception as e:
            logger.error(f"âŒ å¯†é›†é‡‡é›† {i+1}/{len(intervals)} å¤±è´¥: {str(e)}")

    logger.info("ğŸ”´ å¯†é›†é‡‡é›†æ¨¡å¼ç»“æŸ")

@app.get("/telegram-stats")
async def get_telegram_stats():
    """
    è·å–Telegramæ¨é€æ€§èƒ½ç»Ÿè®¡
    ç”¨äºç›‘æ§å¼‚æ­¥æ¨é€çš„æ€§èƒ½è¡¨ç°
    """
    stats = get_telegram_push_stats()

    # è®¡ç®—æˆåŠŸç‡
    success_rate = 0.0
    if stats["total_pushes"] > 0:
        success_rate = (stats["successful_pushes"] / stats["total_pushes"]) * 100

    return {
        "service": "DrawsGuard API Collector v5.2",
        "version": "5.2.0",
        "telegram_push_stats": {
            "total_pushes": stats["total_pushes"],
            "successful_pushes": stats["successful_pushes"],
            "failed_pushes": stats["failed_pushes"],
            "success_rate_percent": round(success_rate, 2),
            "avg_processing_time_seconds": round(stats["avg_processing_time"], 3),
            "max_processing_time_seconds": round(stats["max_processing_time"], 3),
            "min_processing_time_seconds": round(stats["min_processing_time"], 3) if stats["min_processing_time"] != float('inf') else 0,
            "last_push_timestamp": stats["last_push_timestamp"],
            "config": TELEGRAM_PUSH_CONFIG
        },
        "features": [
            "ğŸ° Real-time Telegram push (true async with BackgroundTasks)",
            "ğŸ”„ Retry mechanism with exponential backoff",
            "ğŸ“Š Performance monitoring and statistics",
            "ğŸš¨ Failure notification system",
            "âš¡ Zero-blocking main process",
            "ğŸ¤– Prediction results push to Telegram"
        ],
        "timestamp": datetime.now(SHANGHAI_TZ).isoformat()
    }

@app.get("/health", response_model=HealthCheckResponse, tags=["Monitoring"])
async def health_check():
    """
    Provides a simple health check endpoint.

    This endpoint can be used by load balancers or uptime monitors (like Google
    Cloud Monitoring liveness probes) to verify that the service is running
    and responsive. It returns a simple JSON response with a 200 OK status.
    """
    return HealthCheckResponse(status="ok")

@app.get("/heartbeat")
async def heartbeat():
    """
    å¿ƒè·³ç›‘æ§ç«¯ç‚¹
    
    æ£€æŸ¥é¡¹:
    1. BigQueryè¿æ¥çŠ¶æ€
    2. æœ€è¿‘10åˆ†é’Ÿæ•°æ®æ’å…¥æƒ…å†µ
    3. æœ€è¿‘30åˆ†é’ŸæœŸå·è¿ç»­æ€§
    4. æœåŠ¡å¥åº·åº¦è¯„åˆ†
    
    Returns:
        JSONå“åº”,åŒ…å«è¯¦ç»†çš„å¥åº·çŠ¶æ€
    """
    try:
        bq_client = get_bq_client()

        # 1. æµ‹è¯•BigQueryè¿æ¥
        test_query = "SELECT 1 AS test"
        list(bq_client.query(test_query).result())
        bq_connection = "healthy"

        # 2. æ£€æŸ¥æœ€è¿‘æ’å…¥æƒ…å†µ
        recent_insert_query = """
        SELECT 
          COUNT(*) AS recent_count,
          MAX(created_at) AS last_insert,
          TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), MAX(created_at), SECOND) AS seconds_since_last
        FROM `wprojectl.drawsguard.draws`
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 10 MINUTE)
        """

        insert_result = list(bq_client.query(recent_insert_query).result())[0]
        recent_count = insert_result['recent_count']
        seconds_since_last = insert_result['seconds_since_last']
        last_insert = insert_result['last_insert'].isoformat() if insert_result['last_insert'] else None

        # 3. æ£€æŸ¥æœŸå·è¿ç»­æ€§(æœ€è¿‘30åˆ†é’Ÿ)
        continuity_query = """
        WITH recent AS (
          SELECT CAST(period AS INT64) AS period_int
          FROM `wprojectl.drawsguard.draws`
          WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 MINUTE)
          ORDER BY period_int
        ),
        gaps AS (
          SELECT 
            period_int,
            LEAD(period_int) OVER (ORDER BY period_int) - period_int - 1 AS gap
          FROM recent
        )
        SELECT COUNT(*) AS gap_count
        FROM gaps
        WHERE gap > 0
        """

        continuity_result = list(bq_client.query(continuity_query).result())
        gap_count = continuity_result[0]['gap_count'] if continuity_result else 0

        # 4. è¯„ä¼°å¥åº·çŠ¶æ€
        health_score = 100
        issues = []

        if seconds_since_last > 600:  # 10åˆ†é’Ÿæ— æ•°æ®
            health_score -= 50
            issues.append("10åˆ†é’Ÿå†…æ— æ–°æ•°æ®")
        elif seconds_since_last > 300:  # 5åˆ†é’Ÿæ— æ•°æ®
            health_score -= 20
            issues.append("5åˆ†é’Ÿå†…æ— æ–°æ•°æ®")

        if gap_count > 0:
            health_score -= 30
            issues.append(f"æœ€è¿‘30åˆ†é’Ÿå‘ç°{gap_count}ä¸ªç¼ºå£")

        if health_score >= 90:
            status = "excellent"
        elif health_score >= 70:
            status = "good"
        elif health_score >= 50:
            status = "warning"
        else:
            status = "critical"

        return {
            "status": status,
            "service": "drawsguard-api-collector",
            "version": "5.2.0",
            "health_score": health_score,
            "bigquery_connection": bq_connection,
            "recent_inserts": recent_count,
            "seconds_since_last_insert": seconds_since_last,
            "last_insert": last_insert,
            "gap_count_30min": gap_count,
            "issues": issues if issues else None,
            "telegram_push": "enabled",
            "timestamp": datetime.now(SHANGHAI_TZ).isoformat()
        }

    except Exception as e:
        return {
            "status": "error",
            "service": "drawsguard-api-collector",
            "error": str(e),
            "timestamp": datetime.now(SHANGHAI_TZ).isoformat()
        }

@app.post("/collect")
async def collect(background_tasks: BackgroundTasks):
    """æ™ºèƒ½é‡‡é›†ç«¯ç‚¹"""
    try:
        logger.info("="*60)
        logger.info("å¼€å§‹æ™ºèƒ½é‡‡é›†")
        logger.info("="*60)

        # 1. è·å–APIå¯†é’¥
        api_key = get_api_key()
        logger.info("âœ… APIå¯†é’¥è·å–æˆåŠŸ")

        # 2. å‡†å¤‡APIå‚æ•°
        current_time = str(int(time.time()))
        params = {
            'appid': '45928',
            'format': 'json',
            'time': current_time
        }
        params['sign'] = generate_sign(params, api_key)

        # 3. è°ƒç”¨API(å¸¦é‡è¯•)
        api_url = "https://rijb.api.storeapi.net/api/119/259"
        data = call_api_with_retry(api_url, params, api_key)

        # 4. éªŒè¯è¿”å›æ•°æ®
        if data.get('codeid') != 10000:
            error_msg = f"APIè¿”å›é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}"
            logger.error(error_msg)
            raise HTTPException(status_code=500, detail=error_msg) from None

        # 5. è§£æå¹¶æ’å…¥æ•°æ®
        bq_client = get_bq_client()
        cloud_logger = get_cloud_logger()

        # ç¡®å®šé‡‡é›†æ¨¡å¼
        countdown = data.get('retdata', {}).get('next', {}).get('award_time', 999)
        if 0 < countdown <= INTENSIVE_MODE_THRESHOLD:
            collection_mode = "intensive"
        elif countdown <= ENERGY_SAVE_THRESHOLD:
            collection_mode = "energy_save"
        else:
            collection_mode = "normal"

        result = parse_and_insert_data(data, bq_client, cloud_logger, background_tasks, collection_mode=collection_mode)

        # 6. æ™ºèƒ½è°ƒåº¦é€»è¾‘
        if result.get('success'):
            countdown = result.get('award_countdown', 999)

            # å¯†é›†é‡‡é›†æ¨¡å¼(å¼€å¥–å‰60ç§’)
            if 0 < countdown <= INTENSIVE_MODE_THRESHOLD:
                # ä½¿ç”¨åå°ä»»åŠ¡é¿å…é˜»å¡
                logger.info(f"è§¦å‘å¯†é›†é‡‡é›†åå°ä»»åŠ¡ (countdown={countdown}s)")
                background_tasks.add_task(
                    schedule_intensive_collection,
                    countdown,
                    api_key,
                    bq_client,
                    cloud_logger,
                    background_tasks
                )

            # èŠ‚èƒ½æ¨¡å¼(å¼€å¥–å5åˆ†é’Ÿ)
            elif countdown <= ENERGY_SAVE_THRESHOLD:
                logger.info(f"ğŸ”µ èŠ‚èƒ½æ¨¡å¼:countdown={countdown}ç§’,æ— éœ€é¢å¤–æ“ä½œ")
                cloud_logger.log_text(f"èŠ‚èƒ½æ¨¡å¼: countdown={countdown}", severity='INFO')

        # 7. åŒæ­¥åˆ°pc28.draws (ä¼˜åŒ–: åå°ä»»åŠ¡)
        background_tasks.add_task(sync_to_pc28_draws, result.get('period'))

        logger.info("="*60)
        logger.info("æ™ºèƒ½é‡‡é›†å®Œæˆ")
        logger.info("="*60)

        return {
            "status": "success",
            "timestamp": datetime.now(UTC_TZ).isoformat(),
            "result": result
        }

    except Exception as e:
        logger.error(f"é‡‡é›†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e)) from e

async def sync_to_pc28_draws(period: str):
    """
    å¼‚æ­¥åŒæ­¥å•æœŸæ•°æ®åˆ° pc28.draws (åå°ä»»åŠ¡)
    """
    if not period:
        return

    try:
        bq_client = get_bq_client()
        sync_query = """
        MERGE `wprojectl.pc28.draws` AS target
        USING (
          SELECT * FROM `wprojectl.drawsguard.draws`
          WHERE period = @period
          ORDER BY timestamp DESC
          LIMIT 1
        ) AS source
        ON target.period = source.period
        WHEN NOT MATCHED THEN
          INSERT (period, timestamp, numbers, sum_value, big_small, odd_even)
          VALUES (source.period, source.timestamp, source.numbers, source.sum_value, source.big_small, source.odd_even)
        WHEN MATCHED THEN
          UPDATE SET
            timestamp = source.timestamp,
            numbers = source.numbers,
            sum_value = source.sum_value,
            big_small = source.big_small,
            odd_even = source.odd_even
        """

        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("period", "STRING", period)
            ]
        )
        query_job = bq_client.query(sync_query, job_config=job_config)
        await asyncio.to_thread(query_job.result) # å¼‚æ­¥ç­‰å¾…ç»“æœ

        logger.info(f"âœ… [åå°åŒæ­¥] pc28.drawsæˆåŠŸ: æœŸå·={period}")
    except Exception as e:
        logger.warning(f"âš ï¸ [åå°åŒæ­¥] pc28.drawså¤±è´¥: æœŸå·={period}, é”™è¯¯: {e}")


if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port)

